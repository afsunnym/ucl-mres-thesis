{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d93f63-6a76-4f36-98df-a323c1dd5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import simpledbf\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import h3\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.ops import unary_union\n",
    "import folium\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d228263b-7c8b-4c45-8f98-b4c0f5c3457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and cleaning\n",
    "\n",
    "#OD dataset loading and cleaning\n",
    "dbf = simpledbf.Dbf5('ucl-mres-thesis/OD2023 DATASET-Metro SP/Banco2023_divulgacao_190225.dbf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92bf15f-5553-4213-b301-794f5cf19e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dbf.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8ec3e5-3afb-4527-9b34-097fd16940e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for zero values\n",
    "columns_to_check = [\n",
    "    'CO_O_X', 'CO_O_Y', 'CO_D_X', 'CO_D_Y'\n",
    "]\n",
    "\n",
    "# Keeping only rows where all these columns are non-zero\n",
    "cleaned_df = df[(df[columns_to_check] != 0).all(axis=1)]\n",
    "\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cleaned_df.to_csv(os.path.join(output_dir, 'cleandbf.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b894d81-0705-415c-9513-84fdce5e6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapefile loading and cleaning\n",
    "shapefile_path = \"ucl-mres-thesis/Municipal boundaries-ShapeFiles/Municipios_2023.shp\" \n",
    "gdf = gpd.read_file(shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a22891-45e6-458a-8be1-51081ae33577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "# Ensuring the shapefile is in WGS84 (lat/lon) for H3\n",
    "if gdf.crs.to_epsg() != 4326:\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "gdf_exploded = gdf.explode(index_parts=True)\n",
    "\n",
    "def polygon_to_h3(poly, resolution=9):\n",
    "    geo_json = mapping(poly)\n",
    "    if geo_json[\"type\"] == \"MultiPolygon\":\n",
    "        h3_indexes = set()\n",
    "        for polygon in poly.geoms:\n",
    "            poly_geo = mapping(polygon)\n",
    "            poly_geo[\"coordinates\"] = [\n",
    "                [[y, x] for x, y in ring] for ring in poly_geo[\"coordinates\"][0]\n",
    "            ]\n",
    "            h3_indexes.update(\n",
    "                h3.polyfill(poly_geo, resolution, geo_json_conformant=True)\n",
    "            )\n",
    "        return list(h3_indexes)\n",
    "    elif geo_json[\"type\"] == \"Polygon\":\n",
    "        geo_json[\"coordinates\"] = [\n",
    "            [[y, x] for x, y in ring] for ring in geo_json[\"coordinates\"]\n",
    "        ]\n",
    "        return list(h3.polyfill(geo_json, resolution, geo_json_conformant=True))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported geometry type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763661a1-5884-482d-9fd7-044c926be276",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h3_indexes = set()\n",
    "for poly in gdf_exploded.geometry:\n",
    "    h3_cells = polygon_to_h3(poly, resolution=9)\n",
    "    all_h3_indexes.update(h3_cells)\n",
    "\n",
    "hex_polygons = []\n",
    "for h3_index in all_h3_indexes:\n",
    "    boundary = h3.h3_to_geo_boundary(h3_index, geo_json=True)\n",
    "    hex_polygons.append(Polygon([(lon, lat) for lat, lon in boundary]))\n",
    "\n",
    "hex_gdf = gpd.GeoDataFrame(\n",
    "    {\"h3_index\": list(all_h3_indexes), \"geometry\": hex_polygons}, crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "h3_dir = os.path.join(output_dir, \"H3_res9\")\n",
    "os.makedirs(h3_dir, exist_ok=True)\n",
    "output_path = os.path.join(h3_dir, \"Municipios_2023_h3_res9.shp\")\n",
    "hex_gdf.to_file(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5037f2-2b91-4edd-8836-f1fc0d30216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geting the centroid of the study area for map center\n",
    "center = hex_gdf.geometry.union_all().centroid\n",
    "m = folium.Map(location=[center.y, center.x], zoom_start=12)\n",
    "\n",
    "# Add hexagon boundaries to the map\n",
    "for _, row in hex_gdf.iterrows():\n",
    "    geom = row.geometry\n",
    "    folium.Polygon(\n",
    "        locations=[(y, x) for x, y in zip(*geom.exterior.coords.xy)],\n",
    "        color=\"blue\",\n",
    "        fill=False,\n",
    "    ).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7cedffe-8637-4537-bffb-961bd1f74c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GEO points from the OD DataFrame\n",
    "\n",
    "# Assigning the original CRS: EPSG:22523 (São Paulo - UTM 23S, meters)\n",
    "projected_crs = \"EPSG:22523\"\n",
    "\n",
    "# Create geometry columns for origins and destinations\n",
    "from shapely.geometry import Point\n",
    "df_with_geom = cleaned_df.copy()\n",
    "df_with_geom[\"origin_geom\"] = df_with_geom.apply(lambda row: Point(row[\"CO_O_X\"], row[\"CO_O_Y\"]), axis=1)\n",
    "df_with_geom[\"dest_geom\"] = df_with_geom.apply(lambda row: Point(row[\"CO_D_X\"], row[\"CO_D_Y\"]), axis=1)\n",
    "\n",
    "# Create GeoDataFrames using the starting CRS\n",
    "gdf_origin = gpd.GeoDataFrame(df_with_geom.copy(), geometry=\"origin_geom\", crs=projected_crs)\n",
    "gdf_dest = gpd.GeoDataFrame(df_with_geom.copy(), geometry=\"dest_geom\", crs=projected_crs)\n",
    "\n",
    "# REPROJECT BOTH TO EPSG:4326 for spatial matching with H3 grid\n",
    "gdf_origin = gdf_origin.to_crs(epsg=4326)\n",
    "gdf_dest = gdf_dest.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b8a857-7d28-41c2-af19-ca529c03a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial joins to assign h3 hex indices\n",
    "\n",
    "origin_join = gpd.sjoin(\n",
    "    gdf_origin, hex_gdf[[\"h3_index\", \"geometry\"]], how=\"left\", predicate=\"within\"\n",
    ")\n",
    "df_with_geom[\"h3_origin\"] = origin_join[\"h3_index\"].values\n",
    "\n",
    "dest_join = gpd.sjoin(\n",
    "    gdf_dest, hex_gdf[[\"h3_index\", \"geometry\"]], how=\"left\", predicate=\"within\"\n",
    ")\n",
    "df_with_geom[\"h3_dest\"] = dest_join[\"h3_index\"].values\n",
    "\n",
    "\n",
    "# Making cleaned_df a true copy to allow safe assignment\n",
    "cleaned_df = cleaned_df.copy()\n",
    "\n",
    "cleaned_df.loc[:, \"h3_origin\"] = df_with_geom[\"h3_origin\"].values\n",
    "cleaned_df.loc[:, \"h3_dest\"] = df_with_geom[\"h3_dest\"].values\n",
    "\n",
    "df_with_geom.to_csv(os.path.join(output_dir, 'od_with_h3.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8018c2b1-db80-4a80-9801-f9a0942913ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining 15-minute time windows\n",
    "\n",
    "def get_quarter(hour, minute):\n",
    "    if 0 <= minute < 15:\n",
    "        return f\"{hour:02d}:00–{hour:02d}:14\"\n",
    "    elif 15 <= minute < 30:\n",
    "        return f\"{hour:02d}:15–{hour:02d}:29\"\n",
    "    elif 30 <= minute < 45:\n",
    "        return f\"{hour:02d}:30–{hour:02d}:44\"\n",
    "    else:\n",
    "        return f\"{hour:02d}:45–{hour:02d}:59\"\n",
    "\n",
    "\n",
    "cleaned_df.loc[:, \"quarter_departure\"] = cleaned_df.apply(\n",
    "    lambda row: get_quarter(row[\"H_SAIDA\"], row[\"MIN_SAIDA\"]), axis=1\n",
    ")\n",
    "cleaned_df.loc[:, \"quarter_arrival\"] = cleaned_df.apply(\n",
    "    lambda row: get_quarter(row[\"H_CHEG\"], row[\"MIN_CHEG\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ee1522b-8554-434c-9c35-125d240a75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv(os.path.join(output_dir, 'cleandbf.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca1d890-99d4-40f6-ac5c-5fd02f11d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming 15-minute time windows into indices (0-95)\n",
    "def time_window_to_index(time_window):\n",
    "    # Replace en dash and em dash with hyphen\n",
    "    time_window = time_window.replace('–', '-').replace('—', '-')\n",
    "    # Split on hyphen\n",
    "    parts = time_window.split('-')\n",
    "    start_time = parts[0].strip()\n",
    "    # Check if time is in HH:MM format\n",
    "    if ':' in start_time:\n",
    "        hour, minute = map(int, start_time.split(':'))\n",
    "    else:\n",
    "        # If only hour is present, minute = 0\n",
    "        hour = int(start_time)\n",
    "        minute = 0\n",
    "    return hour * 4 + minute // 15\n",
    "\n",
    "\n",
    "cleaned_df.loc[:, 'departure_idx'] = cleaned_df['quarter_departure'].apply(time_window_to_index)\n",
    "cleaned_df.loc[:, 'arrival_idx'] = cleaned_df['quarter_arrival'].apply(time_window_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad5f0d-953a-4e02-9454-00d5443362bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person-Time_Trip into (PTTI) dataframe creation\n",
    "\n",
    "# Filtering adults and converting hex IDs to string type\n",
    "adults_df = cleaned_df[cleaned_df['IDADE'] >= 18].copy()\n",
    "adults_df['h3_origin'] = adults_df['h3_origin'].astype(str)\n",
    "adults_df['h3_dest'] = adults_df['h3_dest'].astype(str)\n",
    "adults_df['MOTIVO_O'] = adults_df['MOTIVO_O'].astype(str)\n",
    "adults_df['MOTIVO_D'] = adults_df['MOTIVO_D'].astype(str)\n",
    "adults_df['FE_VIA'] = adults_df['FE_VIA'].astype(float)\n",
    "\n",
    "# Creating time indices (ensure this uses the fixed function from earlier)\n",
    "def time_window_to_index(time_window):\n",
    "    time_window = str(time_window).replace('–', '-').replace('—', '-')\n",
    "    start_time = time_window.split('-')[0].strip()\n",
    "    if ':' in start_time:\n",
    "        hour, minute = map(int, start_time.split(':'))\n",
    "    else:\n",
    "        hour, minute = int(start_time), 0\n",
    "    return hour * 4 + minute // 15\n",
    "\n",
    "adults_df['departure_idx'] = adults_df['quarter_departure'].apply(time_window_to_index)\n",
    "adults_df['arrival_idx'] = adults_df['quarter_arrival'].apply(time_window_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "837fd2c8-ece3-4965-8f57-6acf76355d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing person_df with object dtype for hex IDs\n",
    "adult_persons = adults_df['ID_PESS'].unique()\n",
    "person_hex = pd.DataFrame(index=adult_persons, columns=range(96), dtype=object)\n",
    "person_motive = pd.DataFrame(index=adult_persons, columns=range(96), dtype=object)\n",
    "person_fevia = pd.DataFrame(index=adult_persons, columns=range(96), dtype=object)\n",
    "person_hex[:] = None\n",
    "person_motive[:] = None\n",
    "person_fevia[:] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1af6eea2-bb33-436c-8b44-94a361bc41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precomputing first trip info\n",
    "first_trips = (\n",
    "    adults_df.sort_values('departure_idx')\n",
    "    .groupby('ID_PESS')\n",
    "    .head(1)\n",
    "    .set_index('ID_PESS')\n",
    "    [['h3_origin', 'MOTIVO_O', 'departure_idx', 'FE_VIA']]\n",
    ")\n",
    "for person, row in first_trips.iterrows():\n",
    "    dep_idx = row['departure_idx']\n",
    "    if dep_idx > 0:\n",
    "        person_hex.loc[person, :dep_idx-1] = row['h3_origin']\n",
    "        person_motive.loc[person, :dep_idx-1] = row['MOTIVO_O']\n",
    "        person_fevia.loc[person, :dep_idx-1] = row['FE_VIA']\n",
    "\n",
    "# Filling pre-first-trip periods\n",
    "trip_groups = adults_df.sort_values(['ID_PESS', 'departure_idx']).groupby('ID_PESS')\n",
    "for person, trips in trip_groups:\n",
    "    trips = trips.reset_index(drop=True)\n",
    "    for i in range(len(trips)):\n",
    "        arr = trips.loc[i, 'arrival_idx']\n",
    "        if i < len(trips) - 1:\n",
    "            next_dep = trips.loc[i + 1, 'departure_idx']\n",
    "        else:\n",
    "            next_dep = 96\n",
    "        if arr >= next_dep:\n",
    "            continue\n",
    "        person_hex.loc[person, arr:next_dep-1] = trips.loc[i, 'h3_dest']\n",
    "        person_motive.loc[person, arr:next_dep-1] = trips.loc[i, 'MOTIVO_D']\n",
    "        person_fevia.loc[person, arr:next_dep-1] = trips.loc[i, 'FE_VIA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b852cbb-7624-4c61-8f00-08047826ff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56642/1429428910.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  person_fevia = person_fevia.replace([None, np.nan, \"\"], \"transit\")\n"
     ]
    }
   ],
   "source": [
    "# Filling empty slots with \"transit\"\n",
    "\n",
    "person_hex = person_hex.replace([None, np.nan, \"\"], \"transit\")\n",
    "person_motive = person_motive.replace([None, np.nan, \"\"], \"transit\")\n",
    "person_fevia = person_fevia.replace([None, np.nan, \"\"], \"transit\")\n",
    "\n",
    "        \n",
    "# Combining into one MultiIndex DataFrame\n",
    "hex_vals = person_hex.values\n",
    "motive_vals = person_motive.values\n",
    "fevia_vals = person_fevia.values\n",
    "\n",
    "arrays = [np.repeat(range(96), 3), ['hexagon', 'motive', 'tripexpfactor'] * 96]\n",
    "multi_columns = pd.MultiIndex.from_arrays(arrays, names=['quarter', 'info'])\n",
    "\n",
    "combo = np.empty((len(adult_persons), 96 * 3), dtype=object)\n",
    "combo[:, 0::3] = hex_vals\n",
    "combo[:, 1::3] = motive_vals\n",
    "combo[:, 2::3] = fevia_vals\n",
    "\n",
    "final_df = pd.DataFrame(combo, columns=multi_columns, index=adult_persons).reset_index()\n",
    "final_df.rename(columns={'index': 'ID_PESS'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e95934d3-a726-4b3a-9daf-695ab2472fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(os.path.join(output_dir,\"OD_ADULTS_Timeindex_Motive_location_tripexpansionfactors.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
